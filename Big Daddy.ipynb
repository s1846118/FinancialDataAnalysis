{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/320193/000032019319000119/FilingSummary.xml\n",
      "['https://www.sec.gov/Archives/edgar/data/320193/000032019319000119/R2.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019319000119/R4.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019319000119/R5.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019319000119/R7.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019319000119/R55.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019319000119/R60.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019319000119/R70.htm']\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "https://www.sec.gov/Archives/edgar/data/320193/000032019318000145/FilingSummary.xml\n",
      "['https://www.sec.gov/Archives/edgar/data/320193/000032019318000145/R2.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019318000145/R5.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019318000145/R6.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019318000145/R9.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019318000145/R49.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019318000145/R53.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019318000145/R63.htm']\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "https://www.sec.gov/Archives/edgar/data/320193/000032019317000070/FilingSummary.xml\n",
      "['https://www.sec.gov/Archives/edgar/data/320193/000032019317000070/R2.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019317000070/R5.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019317000070/R6.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019317000070/R8.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019317000070/R53.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019317000070/R60.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000032019317000070/R70.htm']\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "https://www.sec.gov/Archives/edgar/data/320193/000162828016020309/FilingSummary.xml\n",
      "['https://www.sec.gov/Archives/edgar/data/320193/000162828016020309/R2.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000162828016020309/R5.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000162828016020309/R6.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000162828016020309/R8.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000162828016020309/R53.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000162828016020309/R60.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000162828016020309/R69.htm']\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "https://www.sec.gov/Archives/edgar/data/320193/000119312515356351/FilingSummary.xml\n",
      "['https://www.sec.gov/Archives/edgar/data/320193/000119312515356351/R2.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312515356351/R5.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312515356351/R6.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312515356351/R8.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312515356351/R54.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312515356351/R63.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312515356351/R72.htm']\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/FilingSummary.xml\n",
      "['https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/R2.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/R5.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/R6.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/R8.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/R39.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/R55.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/R64.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/R74.htm', 'https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/R75.htm']\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n",
      "C:\\Notebooks\\Financials Data\\Company\\320193\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '[1]'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-71b3c2f138dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;31m# everything is a string, so let's convert all the data to a float.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mincome_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mincome_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;31m# Change the column headers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lyndo\\python\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors, **kwargs)\u001b[0m\n\u001b[0;32m   5689\u001b[0m             \u001b[1;31m# else, only a single dtype is given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5690\u001b[0m             new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors,\n\u001b[1;32m-> 5691\u001b[1;33m                                          **kwargs)\n\u001b[0m\u001b[0;32m   5692\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lyndo\\python\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'astype'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lyndo\\python\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m    393\u001b[0m                                             copy=align_copy)\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m             \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lyndo\\python\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'raise'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         return self._astype(dtype, copy=copy, errors=errors, values=values,\n\u001b[1;32m--> 534\u001b[1;33m                             **kwargs)\n\u001b[0m\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m     def _astype(self, dtype, copy=False, errors='raise', values=None,\n",
      "\u001b[1;32mc:\\users\\lyndo\\python\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m_astype\u001b[1;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m                     \u001b[1;31m# _astype_nansafe works fine with 1-d only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m                     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m                 \u001b[1;31m# TODO(extension)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lyndo\\python\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '[1]'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "#Quick url maker\n",
    "def make_url(base_url,comp):\n",
    "    \n",
    "    url = base_url\n",
    "    \n",
    "    # add each component to the base url\n",
    "    for r in comp:\n",
    "        url = '{}/{}'.format(url,r)\n",
    "        \n",
    "    return url\n",
    "\n",
    "#Save function\n",
    "\n",
    "def name_statement_csv(yeet):\n",
    "    if 'Balance Sheets' in yeet:\n",
    "        return 'Balance_Sheet.csv'\n",
    "    elif 'Operations' in yeet:\n",
    "        return 'Operations.csv'\n",
    "    elif 'Comprehensive Loss' in yeet:\n",
    "        return 'Comprensive_Loss.csv'\n",
    "    elif 'Cash Flows' in yeet:\n",
    "        return 'cash_flows.csv'\n",
    "    elif 'BALANCE SHEETS' in yeet:\n",
    "        return 'Balance_Sheet.csv'\n",
    "    elif 'OPERATIONS' in yeet:\n",
    "        return 'Operations.csv'\n",
    "    elif 'COMPREHENSIVE LOSS' in yeet:\n",
    "        return 'Comprensive_Loss.csv'\n",
    "    elif 'CASH FLOWS' in yeet:\n",
    "        return 'cash_flows.csv'\n",
    "    else:\n",
    "        return 'Other.csv'\n",
    "\n",
    "def create_dir(base_dir,comp):\n",
    "\n",
    "    dir_ = base_dir\n",
    "\n",
    "    # add each component to the base url\n",
    "    for r in comp:\n",
    "        dir_ = '{}\\{}'.format(dir_,r)\n",
    "\n",
    "    return dir_\n",
    "\n",
    "#XML version\n",
    "\n",
    "CIK = '320193'\n",
    "\n",
    "endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "\n",
    "param_dict = {'action' : 'getcompany',\n",
    "              'CIK' : '320193',\n",
    "              'type' : '10-K',\n",
    "              'dateb' : '20200421',\n",
    "              'owner' : 'exclude',\n",
    "              'start' : '',\n",
    "              'output' : 'atom',\n",
    "              'count' : '100'}\n",
    "\n",
    "#request url then parse\n",
    "response = requests.get(url = endpoint, params=param_dict)\n",
    "soup = BeautifulSoup(response.content,'lxml')\n",
    "\n",
    "entries = soup.find_all('entry')\n",
    "\n",
    "master_list_xml = []\n",
    "\n",
    "for entry in entries:\n",
    "    \n",
    "    accession_num = entry.find('accession-nunber').text\n",
    "\n",
    "        # create a new dictionary\n",
    "    entry_dict = {}\n",
    "    entry_dict[accession_num] = {}\n",
    "    \n",
    "    # store the category info\n",
    "    category_info = entry.find('category')    \n",
    "    entry_dict[accession_num]['category'] = {}\n",
    "    entry_dict[accession_num]['category']['label'] = category_info['label']\n",
    "    entry_dict[accession_num]['category']['scheme'] = category_info['scheme']\n",
    "    entry_dict[accession_num]['category']['term'] =  category_info['term']\n",
    "\n",
    "    # store the file info\n",
    "    entry_dict[accession_num]['file_info'] = {}\n",
    "    entry_dict[accession_num]['file_info']['file_number'] = entry.find('file-number').text\n",
    "    entry_dict[accession_num]['file_info']['file_number_href'] = entry.find('file-number-href').text\n",
    "    entry_dict[accession_num]['file_info']['filing_date'] =  entry.find('filing-date').text\n",
    "    entry_dict[accession_num]['file_info']['filing_href'] = entry.find('filing-href').text\n",
    "    entry_dict[accession_num]['file_info']['filing_type'] =  entry.find('filing-type').text\n",
    "    entry_dict[accession_num]['file_info']['form_number'] =  entry.find('film-number').text\n",
    "    entry_dict[accession_num]['file_info']['form_name'] =  entry.find('form-name').text\n",
    "    entry_dict[accession_num]['file_info']['file_size'] =  entry.find('size').text\n",
    "    \n",
    "    # store extra info\n",
    "    entry_dict[accession_num]['request_info'] = {}\n",
    "    entry_dict[accession_num]['request_info']['link'] =  entry.find('link')['href']\n",
    "    entry_dict[accession_num]['request_info']['title'] =  entry.find('title').text\n",
    "    entry_dict[accession_num]['request_info']['last_updated'] =  entry.find('updated').text\n",
    "    \n",
    "    # store in the master list\n",
    "    master_list_xml.append(entry_dict)\n",
    "    \n",
    "urls = []\n",
    "\n",
    "base_url = r\"https://www.sec.gov/Archives/edgar/data\"\n",
    "for i in master_list_xml:\n",
    "    for j in i:\n",
    "        j = str(j)\n",
    "        \n",
    "        doc_url = make_url(base_url,[CIK,j.replace(\"-\",\"\"),\"index.json\"])\n",
    "        urls.append(doc_url)\n",
    "        \n",
    "n = 0\n",
    "for url in urls:\n",
    "    content = requests.get(url).json()\n",
    "\n",
    "    for file in content['directory']['item']:\n",
    "        if file['name'] == 'FilingSummary.xml':\n",
    "\n",
    "            xml_summary = r\"https://www.sec.gov\" + content['directory']['name'] + \"/\" + file['name']\n",
    "\n",
    "#            print(file['name'])\n",
    "            print(xml_summary)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    base_url = xml_summary.replace('FilingSummary.xml','')\n",
    "\n",
    "    content = requests.get(xml_summary).content\n",
    "    soup = BeautifulSoup(content,'lxml')\n",
    "\n",
    "    reports = soup.find('myreports')\n",
    "    \n",
    "    master_reports = []\n",
    "    \n",
    "    for report in reports.find_all('report')[:-1]:\n",
    "        report_dict = {}\n",
    "        report_dict['name_short'] = report.shortname.text\n",
    "        report_dict['name_long'] = report.longname.text\n",
    "        try:\n",
    "            report_dict['position'] = report.position.text\n",
    "        except:\n",
    "            report_dict['position'] = 'NONE'\n",
    "        try:\n",
    "            report_dict['category'] = report.menucategory.text\n",
    "        except:\n",
    "            report_dict['category'] = 'NONE'\n",
    "        report_dict['url'] = base_url + report.htmlfilename.text\n",
    "    \n",
    "        master_reports.append(report_dict)\n",
    "\n",
    "\n",
    "# create the list to hold the statement urls\n",
    "    statements_url = []\n",
    "\n",
    "    for report_dict in master_reports:\n",
    "    \n",
    "        # define the statements we want to look for.\n",
    "        item1 = r\"Balance Sheets\"\n",
    "        item2 = r\"Operations and Comprehensive Income (Loss)\"\n",
    "        item3 = r\"Cash Flows\"\n",
    "        item4 = r\"Stockholder's Equity\"\n",
    "        item5 = r\"Operations\"\n",
    "        item6 = r\"Comprehensive Loss\"\n",
    "        \n",
    "        ITEM1 = r\"BALANCE SHEETS\"\n",
    "        ITEM2 = r\"OPERATIONS AND COMPREHENSIVE INCOME\"\n",
    "        ITEM3 = r\"CASH FLOWS\"\n",
    "        ITEM4 = r\"STOCKHOLDER'S EQUITY\"\n",
    "        ITEM5 = r\"OPERATIONS\"\n",
    "        ITEM6 = r\"COMPREHENSIVE LOSS\"\n",
    "\n",
    "        # store them in a list.\n",
    "        report_list = [item1, item2, item3, item4,item5,item6]\n",
    "        REPORT_LIST = [ITEM1,ITEM2,ITEM3,ITEM4,ITEM5,ITEM6]\n",
    "\n",
    "        for i in range(len(report_list)):    # if the short name can be found in the report list.\n",
    "            if report_list[i] in report_dict['name_short']:\n",
    "\n",
    "                statements_url.append(report_dict['url'])\n",
    "\n",
    "            elif REPORT_LIST[i] in report_dict['name_short']:\n",
    "\n",
    "                statements_url.append(report_dict['url'])\n",
    "            \n",
    "    print(statements_url)\n",
    "\n",
    "    # let's assume we want all the statements in a single data set.\n",
    "    statements_data = []\n",
    "\n",
    "    # loop through each statement url\n",
    "    for statement in statements_url:\n",
    "\n",
    "        # define a dictionary that will store the different parts of the statement.\n",
    "        statement_data = {}\n",
    "        statement_data['headers'] = []\n",
    "        statement_data['sections'] = []\n",
    "        statement_data['data'] = []\n",
    "\n",
    "        # request the statement file content\n",
    "        content = requests.get(statement).content\n",
    "        report_soup = BeautifulSoup(content, 'html')\n",
    "\n",
    "\n",
    "        # find all the rows, figure out what type of row it is, parse the elements, and store in the statement file list.\n",
    "        for index, row in enumerate(report_soup.table.find_all('tr')):\n",
    "\n",
    "            # first let's get all the elements.\n",
    "            cols = row.find_all('td')\n",
    "\n",
    "            # if it's a regular row and not a section or a table header\n",
    "            if (len(row.find_all('th')) == 0 and len(row.find_all('strong')) == 0): \n",
    "                reg_row = [ele.text.strip() for ele in cols]\n",
    "                statement_data['data'].append(reg_row)\n",
    "\n",
    "            # if it's a regular row and a section but not a table header\n",
    "            elif (len(row.find_all('th')) == 0 and len(row.find_all('strong')) != 0):\n",
    "                sec_row = cols[0].text.strip()\n",
    "                statement_data['sections'].append(sec_row)\n",
    "\n",
    "            # finally if it's not any of those it must be a header\n",
    "            elif (len(row.find_all('th')) != 0):            \n",
    "                hed_row = [ele.text.strip() for ele in row.find_all('th')]\n",
    "                statement_data['headers'].append(hed_row)\n",
    "\n",
    "            else:            \n",
    "                print('We encountered an error.')\n",
    "\n",
    "        # append it to the master list.\n",
    "        statements_data.append(statement_data)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Grab the proper components\n",
    "\n",
    "    for statement in range(len(statements_data)):\n",
    "        try:\n",
    "            income_header =  statements_data[statement]['headers'][1]\n",
    "        except:\n",
    "            income_header = statements_data[statement]['headers'][0][-2:]\n",
    "\n",
    "        income_data = statements_data[statement]['data']\n",
    "        category = statements_data[statement]['headers'][0][0]\n",
    "\n",
    "        # Put the data in a DataFrame\n",
    "        income_df = pd.DataFrame(income_data)\n",
    "\n",
    "\n",
    "        # Define the Index column, rename it, and we need to make sure to drop the old column once we reindex.\n",
    "        income_df.index = income_df[0]\n",
    "        income_df.index.name = category.replace(\"$\",\"\").replace(\"()\",\"\")\n",
    "        income_df = income_df.drop(0, axis = 1)\n",
    "\n",
    "        # Get rid of the '$', '(', ')', and convert the '' to NaNs.\n",
    "        income_df = income_df.replace('[\\$,)]','', regex=True )\\\n",
    "                             .replace( '[(]','-', regex=True)\\\n",
    "                             .replace( '', 'NaN', regex=True)\n",
    "\n",
    "        # everything is a string, so let's convert all the data to a float.\n",
    "        income_df = income_df.astype(float)\n",
    "\n",
    "        # Change the column headers\n",
    "        income_df.columns = income_header\n",
    "    \n",
    "\n",
    "\n",
    "        for key in master_list_xml[n]:\n",
    "            key = key\n",
    "\n",
    "        date = master_list_xml[n][str(key)]['file_info']['filing_date']\n",
    "        \n",
    "        u = r\"C:\\Notebooks\\Financials Data\\Company\"\n",
    "        r = [CIK]\n",
    "        if not os.path.exists(create_dir(u,r)):\n",
    "            os.mkdir(create_dir(u,r))\n",
    "            \n",
    "        print(create_dir(u,r))\n",
    "        u = r\"C:\\Notebooks\\Financials Data\\Company\"\n",
    "        r = [CIK,date]\n",
    "        if not os.path.exists(create_dir(u,r)):\n",
    "            os.mkdir(create_dir(u,r))\n",
    "\n",
    "        u = r\"C:\\Notebooks\\Financials Data\\Company\"\n",
    "        r = [CIK,date,name_statement_csv(category)]\n",
    "        income_df.to_csv(create_dir(u,r))\n",
    "        \n",
    "    n += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [r\"Balance Sheets\"]\n",
    "\n",
    "if list[0] in \"Consolidated Balance Sheets\":\n",
    "    print('yeet')\n",
    "elif \"Consolidated Balance Sheets\" in list:\n",
    "    print('yeet')\n",
    "else:\n",
    "    print(\"no yeet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
